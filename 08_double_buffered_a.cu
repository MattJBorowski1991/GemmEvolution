// 08_final_double_buffered.cu
// Double-buffered shared memory + Tensor Cores → 1,500–1,900+ TFLOPS
// This is the final form — used (in spirit) by cuBLAS, CUTLASS, FlashAttention

#include "common/utils.cuh"
#include <mma.h>                //Enables Tensor Core instructions
using namespace nvcuda;         // Brings wmma:: into scope

// --- Tiling and launch constants ---
#define WMMA_M 16
#define WMMA_K 16
#define WMMA_N 16
#define PAD 0
#define WARPS_PER_BLOCK 8
#define WARP_TILES_X 4
#define WARP_TILES_Y 2



__global__ void double_buffered_kernel(
    float alpha,
    const half* A,
    const half* B,
    float beta,
    float* C,
    int M, int N, int K
)
{
    // Use macros for all tiling/launch constants
    int tid = threadIdx.x;
    int warp_id = tid / 32;
    int lane_id = tid % 32;
    // Layout warps in a 4x2 grid of tiles per block
    int warp_tile_row = warp_id / WARP_TILES_X;
    int warp_tile_col = warp_id % WARP_TILES_X;
    const int tile_row = blockIdx.y * (WMMA_M * WARP_TILES_Y) + warp_tile_row * WMMA_M;
    const int tile_col = blockIdx.x * (WMMA_N * WARP_TILES_X) + warp_tile_col * WMMA_N;
    if (tile_row >= M || tile_col >= N) return;


    //********DOUBLE BUFFER START ********

    // Per-warp double buffers in shared memory
    __shared__ __align__(16) half A_s[2][WARPS_PER_BLOCK][WMMA_M][WMMA_K + PAD];
    __shared__ __align__(16) half B_s[2][WARPS_PER_BLOCK][WMMA_K][WMMA_N + PAD];

    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;
    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;
    wmma::fill_fragment(c_frag, 0.0f);

    //current buffer (0 or 1)
    int buf = 0;

    // Load first tile from global to current buffer in shared
    // wmma::load_matrix_sync() cannot read directly from global to fragments!! 
    // Hence you need to load the tiles manually (but NOT with nested forloops)
    // Cooperative initial load: each warp loads its own 16x16 tile
    for (int i = lane_id; i < WMMA_M * WMMA_K; i += 32) {
        int m = i / WMMA_K;
        int k = i % WMMA_K;
        A_s[buf][warp_id][m][k] = A[(tile_row + m) * K + k];
    }
    for (int i = lane_id; i < WMMA_K * WMMA_N; i += 32) {
        int k = i / WMMA_N;
        int n = i % WMMA_N;
        B_s[buf][warp_id][k][n] = B[k * N + (tile_col + n)];
    }
    __syncthreads();        //add everytime all threads write TO (not from) shared memory

    // load first tile from shared to fragments
    // NOTE: the address is missaligned if you do PAD=1, as then the offsets will be every (17 halfs x 2 bytes) = 34 bytes. Need to set PAD to a number that will guarantee 16 byt alignment e.g 0 or 8
    wmma::load_matrix_sync(a_frag, &A_s[buf][warp_id][0][0], WMMA_K + PAD);
    wmma::load_matrix_sync(b_frag, &B_s[buf][warp_id][0][0], WMMA_N + PAD);

    // Main loop: overlap load (to/from buffers) with compute (in fragments)
    for (int k = WMMA_K; k < K; k += WMMA_K){
        int next = 1 - buf;

        // Load next tile into the other buffer

        // the below brute-force loading with nested forloops means 
        // that each of 32 threads load the ENTIRE 16x16 tile instead of cooperative loading
        // = 32x redundant memory traffic and serialized execution
        for (int m = 0; m < WMMA_M; ++m)
            for (int kk = 0; kk < WMMA_K; ++kk)
                A_s[next][warp_id][m][kk] = A[(tile_row + m) * K + k + kk];
        for (int kk = 0; kk < WMMA_K; ++kk)
            for (int n = 0; n < WMMA_N; ++n)
                B_s[next][warp_id][kk][n] = B[(k + kk) * N + (tile_col + n)];
        
        __syncthreads();
        

        // compute current tile;
        wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

        buf = next;
        wmma::load_matrix_sync(a_frag, &A_s[buf][warp_id][0][0], WMMA_K + PAD);
        wmma::load_matrix_sync(b_frag, &B_s[buf][warp_id][0][0], WMMA_N + PAD);
    }

    //compute last tile 
    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

    //********DOUBLE BUFFER END ********

    // Apply alpha and beta*C
    float* c_dst = C + tile_row * N + tile_col;
    
    if (beta != 0.0f) {
        wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_old;
        wmma::load_matrix_sync(c_old, c_dst, N, wmma::mem_row_major);
        for (int i = 0; i < c_frag.num_elements; ++i)
            c_frag.x[i] = alpha * c_frag.x[i] + beta * c_old.x[i];
    } else {
        for (int i = 0; i < c_frag.num_elements; ++i)
            c_frag.x[i] = alpha * c_frag.x[i];
    }
    
    wmma::store_matrix_sync(c_dst, c_frag, N, wmma::mem_row_major);
}

// Wrapper matching run_gemm_test_wmma expected (M,K,N order) -> kernel uses (M,N,K)
void launch_double_buffered_tc(float alpha, const __half* A, const __half* B, float beta, float* C, int M, int K, int N) {
    // Derive launch configuration from M,N,K
    dim3 block(32 * WARPS_PER_BLOCK);
    dim3 grid((N + (WMMA_N * WARP_TILES_X) - 1) / (WMMA_N * WARP_TILES_X),
              (M + (WMMA_M * WARP_TILES_Y) - 1) / (WMMA_M * WARP_TILES_Y));
    double_buffered_kernel<<<grid, block>>>(alpha, A, B, beta, C, M, N, K);
    CHECK_CUDA(cudaGetLastError());
}

int main(){
    std::cout << "===08_double_buffered (wmma runner)===\n";
    for (int s : TEST_SIZE){
        run_gemm_test_wmma("08_double_buffered", launch_double_buffered_tc, s, s, s,
                   /*blockDim=*/32 * WARPS_PER_BLOCK, /*dynamicSmemBytes=*/0,
                   (const void*)double_buffered_kernel);
    }
    return 0;
}

// nvcc -O3 -arch=sm_86 08_double_buffered_a.cu -o 08_double_buffered_a && ./08_double_buffered_a | tee 08_double_buffered_a.txt